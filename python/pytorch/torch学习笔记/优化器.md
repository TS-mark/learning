# optimizer
```python
import torch

optimizer = torch.optim.SGD(model_name, lr = lr, momentum = 0.9, weight_decay = 5e-4)
```
#### optimizer基本参数
|name|解释|
|-|-|
|defaults|优化器超参数|
|state|参数缓存，例如momentum缓存|
|param_groups|管理的参数组|
|_step_count|记录更新次数|

#### optimizer的基本使用方法

|name|解释|
|-|-|
|zero_grad()|清空所有参数的梯度|
|step()|进行更新|
|add_param_groups()|添加一组参数|
|state_dict()|获取优化器当前状态信息字典|
|load_state_dict()|加载状态信息字典|

官方文档有说明个别方法使用foreach和for loop，foreach更快，因为使用了张量的计算。有一些使用fused（融合）的优化器计算起来更快


## 优化器（torch官方文档算法）


---

### SGD
$$
\begin{aligned}
    &\rule{110mm}{0.4pt}                                                                 \\
    &\textbf{input}      : \gamma \text{ (lr)}, \: \theta_0 \text{ (params)}, \: f(\theta)
        \text{ (objective)}, \: \lambda \text{ (weight decay)},                          \\
    &\hspace{13mm} \:\mu \text{ (momentum)}, \:\tau \text{ (dampening)},
    \:\textit{ nesterov,}\:\textit{ maximize}                                     \\[-1.ex]
    &\rule{110mm}{0.4pt}                                                                 \\
    &\textbf{for} \: t=1 \: \textbf{to} \: \ldots \: \textbf{do}                         \\
    &\hspace{5mm}g_t           \leftarrow   \nabla_{\theta} f_t (\theta_{t-1})           \\
    &\hspace{5mm}\textbf{if} \: \lambda \neq 0                                           \\
    &\hspace{10mm} g_t \leftarrow g_t + \lambda  \theta_{t-1}                            \\
    &\hspace{5mm}\textbf{if} \: \mu \neq 0                                               \\
    &\hspace{10mm}\textbf{if} \: t > 1                                                   \\
    &\hspace{15mm} \textbf{b}_t \leftarrow \mu \textbf{b}_{t-1} + (1-\tau) g_t           \\
    &\hspace{10mm}\textbf{else}                                                          \\
    &\hspace{15mm} \textbf{b}_t \leftarrow g_t                                           \\
    &\hspace{10mm}\textbf{if} \: \textit{nesterov}                                       \\
    &\hspace{15mm} g_t \leftarrow g_{t} + \mu \textbf{b}_t                             \\
    &\hspace{10mm}\textbf{else}                                                   \\[-1.ex]
    &\hspace{15mm} g_t  \leftarrow  \textbf{b}_t                                         \\
    &\hspace{5mm}\textbf{if} \: \textit{maximize}                                          \\
    &\hspace{10mm}\theta_t \leftarrow \theta_{t-1} + \gamma g_t                   \\[-1.ex]
    &\hspace{5mm}\textbf{else}                                                    \\[-1.ex]
    &\hspace{10mm}\theta_t \leftarrow \theta_{t-1} - \gamma g_t                   \\[-1.ex]
    &\rule{110mm}{0.4pt}                                                          \\[-1.ex]
    &\bf{return} \:  \theta_t                                                     \\[-1.ex]
    &\rule{110mm}{0.4pt}                                                          \\[-1.ex]
\end{aligned}
$$
Nesterov momentum is based on the formula from
`On the importance of initialization and momentum in deep learning`__.

* 参数:
    * params (iterable): iterable of parameters to optimize or dicts defining parameter groups
    * lr (float): learning rate
    * momentum (float, optional): momentum factor (default: 0)
    * weight_decay (float, optional): weight decay (L2 penalty) (default: 0)
    * dampening (float, optional): dampening for momentum (default: 0)
    * nesterov (bool, optional): enables Nesterov momentum (default: False)
    * maximize (bool, optional): maximize the params based on the objective, instead of minimizing (default: False)
    * foreach (bool, optional): whether foreach implementation of optimizer is used (default: None)

* Example:
```python
optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)
optimizer.zero_grad()
loss_fn(model(input), target).backward()
optimizer.step()
```
__ http://www.cs.toronto.edu/%7Ehinton/absps/momentum.pdf


SGD与Momentum/Nesterov的实现与Sutskever等人略有不同。以及其他一些框架中的实现。

考虑到动量的具体情况，更新可以写成:
.. math::
$$
    \begin{aligned}
        v_{t+1} & = \mu * v_{t} + g_{t+1}, \\
        p_{t+1} & = p_{t} - \text{lr} * v_{t+1},
    \end{aligned}
$$
where :$p$, $g$, $v$ and $\mu$ 分别表示为 parameters(参数), gradient（梯度）, velocity（速度）, and momentum（动量） 

这与Sutskever和其他使用升级形式的框架 形成鲜明对比

$$
    \begin{aligned}
        v_{t+1} & = \mu * v_{t} + \text{lr} * g_{t+1}, \\
        p_{t+1} & = p_{t} - v_{t+1}.
    \end{aligned}
$$
Nesterov版本进行了类似的修改。