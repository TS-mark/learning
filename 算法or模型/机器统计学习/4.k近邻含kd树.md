距离：  
$x_i = (x_i^{(1)},x_i^{(2)},x_i^{(3)},...,x_i^{(n)})$  
欧氏距离（二范数）：常用  
$$L_2(x_i,x_j) = (\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)}|^2)^{\frac{1}{2}}$$

曼哈顿距离（一范数）：可能不太行
$$L_1(x_i,x_j) = \sum_{l=1}^n|x_i^{(l)}-x_j^{(l)}|$$

p范数：(不咋用)
$$L_2(x_i,x_j) = (\sum_{l=1}^n|x_i^{(l)}-x_j^{(l)}|^p)^{\frac{1}{p}}$$

思想（分类决策规则：多数表决）：<font color = red>根据最近k个邻节点</font>，判定预测结果  
没有训练过程，直接算距离就完事了
#### k值较小：
预测结果对临近的实例点非常敏感。如果临近是噪声，预测会出错

#### 较大k值：
预测结果会出错
#### k值选择：
先选取较小k值，使用验证集进行交叉验证

### kd树
比k近邻牛逼点的算法  
构造平衡kd树  
输入：k维空间（这个k跟k近邻不太一样）数据集$T = \{x_1, x_2,...x_N \}$,其中$x_i = (x_i^1,x_i^2,x_i^3,...,x_i^k)^T$
输出：kd树  
思想：将所有样本的某一个维度i取中位数，然后左边分为一类，右边分为一类（还有线上的样本点，不去管他了）
如果区域有样本点，继续划分，直到所有样本点都在超矩形的边界上  
可以加快搜索速度，减少计算距离的次数  
效率：o(logN)
