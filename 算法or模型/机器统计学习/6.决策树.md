## 信息熵：
熵表示随机变量不确定性的度量
其概率分布： $P(X = x_i) = p_i$，pi是概率    
$$H（x） = -\sum_{i=1}^np_ilog p_i$$  
看公式就知道：均匀分布的时候熵最大，熵越大，表示混乱  

优化目标：在最小化信息熵  
信息增益：表示得知特征x的信息而使得类y的信息不确定的减小程度  
特征A对训练集D的信息增益$g(D，A)$，定义为集合D的经验熵H（D）与特征A给定条件下D的经验条件熵H（D|A）之差
$$g(D,A) = H(D) - H(D|A)$$

---
## 信息增益算法：  
输入，训练集d和特征a  
输出，特征A对训练数据集D的信息增益g(d,a)  
1) 计算数据集D的经验熵H(D)
  $$ p_{C_k} = \frac{|C_k|}{D}$$ 
  这里$C_k$很好理解，就是某类的数目
  $$H(D) = -\sum_{k=1}^kp_{C_k}log(p_{C_k})$$
2) 计算特征A对数据集D的经验条件熵H(D|A)  
 $$ H(D|A)= -\sum_{i=1}^N \frac{|D_i|}{|D|}H(D_i)\\
$$
3） 计算信息增益

$$g(D,A) = H(D) - H(D|A)$$

---
## 信息增益比
$$ g_r(D,A) = \frac{g(D,A)}{H_A(D)} $$

---

ID3:信息增益  
C4.5:信息增益比（常用）  

---

总结：  
1. 核心思想，以树结构为基础，每个节点对某特征进行判断，进入分支，直到到达叶节点  
2. 决策树构造的核心思想：让信息熵快速下降，从而达到最少的判断次数获得标签
3. 判断信息熵下降速度的方法：信息增益
4. 构建决策树算法：ID3、C4.5
5. （面试：为什么不用信息增益）信息增益会导致节点偏向选取取值较多的特征问题（另：大数会导致概率准确问题）

---
## 树的剪枝

    过拟合原因：
1) train太久。。。early stop
2) 模型太复杂(经常体现在神经网络中)

后剪枝：模型构造完后剪枝
预剪枝：融合到代价函数中，在模型运行时剪枝

树T的叶节点个数为T，t是树T的叶节点，该叶节点有$N_t$个样本点，其中k类的样本点有$N_{tk}$个，$H_t(T)$是叶节点t上的经验熵，$\alpha \ge 0$为参数，则决策树学习的损失函数可以定义为：
$$C_\alpha(T) = \sum_{t=1}^T N_t H_t (T) + \alpha T$$
其中$H(T) = -\sum_k \frac{N_{tk}}{N_t}log\frac{N_{tk}}{N_t}$
可以理解为$\alpha T$是正则项

带剪枝的模型：CART