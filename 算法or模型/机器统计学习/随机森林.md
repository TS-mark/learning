# 随机森林

---

集成学习分类：bagging

- [ ] 优点
1. 随机森林算法能解决分类和回归两种类型的问题，表现良好，集成学习，方差和偏差都比较低，泛化性能优越
2. 随机森林对高维数据集的处理能力很好，它可以处理成千上万的输入变量，并确定最重要的变量，因此被认为是一个不错的降维方法
3. 可以应对缺失数据
4. 当存在分类不平衡的情况，随机森林能提供平衡数据集误差的有效方法
5. 高度并行化，易于分布实现
6. 树模型，不需要归一化即可使用

- [ ] 缺点
1. 解决回归问题时没有分类中表现好（就适合分类呗）
2. 随机森林对于训练者：无法控制模型内部的运行，只能在不同的参数和随机种子之间进行尝试
3. 忽略属性之间相关性

## 参数

调参方法：
grid search 网格搜索。
sklearn提供

n_estimators越多结果越稳定—>增加计算量<br>
分裂条件：参数需要在实际运用时灵活调整-->gini、信息增益<br>
每棵树最大特征数(max_features)：sqrt（总特征数）<br>
最大叶节点数(max_leaf_nodes)以及“最大树深度”(max_depth)，可以粗粒度的调整树的结构：<br>
叶节点越多或者树越深，意味子模型的偏差越低，方差越高；<br>
调整“分裂所需最小样本数”(min_samples_split)、“叶节点最小样本数”(min_samples_leaf)以及“叶节点最小权重总值”<br>(min_weight_fraction_leaf),可以更细粒度的调整树的结构：分裂所需样本数越少或者叶节点所需样本越少，也意味着子模型越复杂。<br>