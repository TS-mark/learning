{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自注意力机制\n",
    "\n",
    "[参考文档](https://zhuanlan.zhihu.com/p/631398525?utm_id=0)\n",
    "大概这样的：\n",
    "输入：\n",
    "1. X：N x H\n",
    "2. $ W^K 、W^Q、W^V $：H x H ->可训练的共享参数\n",
    "3. X x $ W^K 、W^Q、W^V $ -> K 、 Q 、 V： N x H\n",
    "4. $ \\alpha = softmax( Q \\text{x} K^T )-->dim = -1 $ : N x N\n",
    "5. $ \\alpha $ x V: N x H"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./img/attention_self1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.d_kq = embed_dim\n",
    "        self.K = nn.Linear(embed_dim,embed_dim)\n",
    "        self.Q = nn.Linear(embed_dim,embed_dim)\n",
    "        self.V = nn.Linear(embed_dim,embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        key = self.K(x)\n",
    "        query = self.Q(x)\n",
    "        value = self.V(x)\n",
    "        attention_scores = torch.matmul(query,key.T)/torch.sqrt(self.d_kq)\n",
    "        attention_scores = nn.functional.softmax(attention_scores,dim=-1)\n",
    "        self_attention = torch.matmul(attention_scores,value)\n",
    "        return self_attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多头注意力机制\n",
    "\n",
    "多头注意力机制和自注意力机制区别：\n",
    "1. 它通过使用多个独立的注意力头，分别计算注意力权重，并将它们的结果进行拼接或加权求和，从而获得更丰富的表示。\n",
    "  在自注意力机制中，每个单词或者字都仅仅只有一个 q、k、v与其对应，如下图所示：\n",
    "  $$q^i = W^q a^i$$\n",
    "  ![](./img/attention_self2.png)\n",
    "2. 多头注意力机制在$a^i$乘q、k、v之后再分配多个q、k、v\n",
    "在$$q^i = W^q a^i$$后继续分配多个head，例如分配俩head，则以q为例的两个头：\n",
    "$$q_{i,1} = W_{q,1}q^i$$\n",
    "$$q_{i,2} = W_{q,2}q^i$$\n",
    "$$q_{i,1} \\cdot k_{i,1}^T$$\n",
    "\n",
    "![](./img/attention_multi1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# 定义多头自注意力模块\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "        self.fc = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, embed_dim = x.size()\n",
    "\n",
    "        # 将输入向量拆分为多个头\n",
    "        q = self.query(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        k = self.key(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        v = self.value(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # 计算注意力权重\n",
    "        attn_weights = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float))\n",
    "        attn_weights = torch.softmax(attn_weights, dim=-1)\n",
    "\n",
    "        # 注意力加权求和\n",
    "        attended_values = torch.matmul(attn_weights, v).transpose(1, 2).contiguous().view(batch_size, seq_len, embed_dim)\n",
    "\n",
    "        # 经过线性变换和残差连接\n",
    "        x = self.fc(attended_values) + x\n",
    "\n",
    "        return x\n",
    "\n",
    "# 定义多头自注意力分类器模型\n",
    "class MultiHeadSelfAttentionClassifier(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, hidden_dim, num_classes):\n",
    "        super(MultiHeadSelfAttentionClassifier, self).__init__()\n",
    "        self.attention = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.attention(x)\n",
    "        x = x.mean(dim=1)  # 对每个位置的向量求平均\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, num_head, emb_dim):\n",
    "        self.dim = emb_dim // num_head\n",
    "        self.q = nn.Linear(self.dim,self.dim)\n",
    "        self.k = nn.Linear(self.dim,self.dim)\n",
    "        self.v = nn.Linear(self.dim,self.dim)\n",
    "        self.embed_dim = emb_dim\n",
    "        self.head_num = num_head\n",
    "    def forward(self,x):\n",
    "        batch_size, seq_len, embed_dim = x.size()\n",
    "        if embed_dim != self.dim:\n",
    "            assert \"input dim 不等于初始化dim\"\n",
    "        q = self.q(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10, 2, 15])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# x= torch.random()\n",
    "x = torch.rand(3,10,20)\n",
    "fc = torch.nn.Linear(20,30)\n",
    "res = fc(x) # torch.Size([3, 10, 30])\n",
    "res = res.view(3,10,2,15)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (30x20 and 10x30)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39m# x = torch.rand(3,10,20)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m fc \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mLinear(\u001b[39m10\u001b[39m,\u001b[39m30\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m res \u001b[39m=\u001b[39m fc(x)\n",
      "File \u001b[1;32mc:\\Users\\mark\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\mark\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (30x20 and 10x30)"
     ]
    }
   ],
   "source": [
    "x = torch.rand(3,10,20)\n",
    "# x = torch.rand(3,10,20)\n",
    "fc = torch.nn.Linear(10,30)\n",
    "res = fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3249, 0.5755, 0.1321, 0.3219, 0.5015, 0.4152, 0.6919, 0.4683, 0.3959,\n",
      "         0.8583],\n",
      "        [0.0333, 0.6990, 0.1000, 0.4782, 0.4039, 0.6993, 0.9004, 0.9163, 0.3077,\n",
      "         0.1099],\n",
      "        [0.6061, 0.3521, 0.1836, 0.2257, 0.9928, 0.6356, 0.8696, 0.2656, 0.3837,\n",
      "         0.3558],\n",
      "        [0.2074, 0.1046, 0.0048, 0.5473, 0.2107, 0.6902, 0.9560, 0.6407, 0.3129,\n",
      "         0.6873]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3249, 0.5755, 0.1321, 0.3219, 0.5015, 0.4152, 0.6919, 0.4683,\n",
       "          0.3959, 0.8583],\n",
       "         [0.0333, 0.6990, 0.1000, 0.4782, 0.4039, 0.6993, 0.9004, 0.9163,\n",
       "          0.3077, 0.1099]],\n",
       "\n",
       "        [[0.6061, 0.3521, 0.1836, 0.2257, 0.9928, 0.6356, 0.8696, 0.2656,\n",
       "          0.3837, 0.3558],\n",
       "         [0.2074, 0.1046, 0.0048, 0.5473, 0.2107, 0.6902, 0.9560, 0.6407,\n",
       "          0.3129, 0.6873]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(4,10)\n",
    "print(a)\n",
    "a.view(2,2,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
