# 转置卷积(Transposed Convolution)

转置卷积首先也是一种卷积操作，绝大部分转置卷积是为了实现<font color = "red">上采样</font>的目的，之所以使用转置卷积代替其他上采样方法(最邻近插值、双线性插值)，是因为转置卷积有可学习的参数，可以让网络学到最优上采样的方法。
## 转置卷积操作
首先，先来定义一些变量。

* 输入矩阵宽和高分别是：Win，Hin；
* 输出矩阵宽和高分别为：Wout, Hout；
* 步长为stride，简记为s；
* 填充像素数padding，简记为p；
* 卷积核大小kernel size，简记为k。

---

运算步骤：  
* 在输入矩阵的行间和列间分别填充s-1个0；  
* 在输入矩阵的四周，分别填充k-p-1行(列)0；  
* 卷积操作。  
(1) s = 1,p = 0, k = 3
![](./img/ConvTransposeS1P0K3.gif)
(2) s = 2,p = 0, k = 3
![](./img/ConvTransposeS2P0K3.gif)
(2) s = 2,p = 1, k = 3
这里pad为1，所以输入图像自然就是3\*3才能乘回5\*5.
![](./img/ConvTransposeS2P1K3.gif)

pytorch中H、W的计算挖出来了(BCHW里面那个hw)
 math
 $$
H_{out} = (H_{in} - 1) \times \text{stride}[0] - 2 \times \text{padding}[0] + \text{dilation}[0]\times (\text{kernel\_size}[0] - 1) + \text{output\_padding}[0] + 1
$$
math
$$
W_{out} = (W_{in} - 1) \times \text{stride}[1] - 2 \times \text{padding}[1] + \text{dilation}[1]\times (\text{kernel\_size}[1] - 1) + \text{output\_padding}[1] + 1
$$