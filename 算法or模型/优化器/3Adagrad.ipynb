{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "optimizer = torch.optim.Adagrad()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 优点：适应性学习率，适合处理稀疏数据，可以适应不同参数的不同更新频率  \n",
    "* 缺点：学习率可能在训练过程中过早减小，导致收敛缓慢或停止。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "    &\\rule{110mm}{0.4pt}                                                                 \\\\\n",
    "    &\\textbf{input}      : \\gamma \\text{ (lr)}, \\: \\theta_0 \\text{ (params)}, \\: f(\\theta)\n",
    "        \\text{ (objective)}, \\: \\lambda \\text{ (weight decay)},                          \\\\\n",
    "    &\\hspace{12mm}    \\tau \\text{ (initial accumulator value)}, \\: \\eta\\text{ (lr decay)}\\\\\n",
    "    &\\textbf{initialize} :  state\\_sum_0 \\leftarrow 0                             \\\\[-1.ex]\n",
    "    &\\rule{110mm}{0.4pt}                                                                 \\\\\n",
    "    &\\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}                         \\\\\n",
    "    &\\hspace{5mm}g_t           \\leftarrow   \\nabla_{\\theta} f_t (\\theta_{t-1})           \\\\\n",
    "    &\\hspace{5mm} \\tilde{\\gamma}    \\leftarrow \\gamma / (1 +(t-1) \\eta)                  \\\\\n",
    "    &\\hspace{5mm} \\textbf{if} \\: \\lambda \\neq 0                                          \\\\\n",
    "    &\\hspace{10mm} g_t \\leftarrow g_t + \\lambda \\theta_{t-1}                             \\\\\n",
    "    &\\hspace{5mm}state\\_sum_t  \\leftarrow  state\\_sum_{t-1} + g^2_t                      \\\\\n",
    "    &\\hspace{5mm}\\theta_t \\leftarrow\n",
    "        \\theta_{t-1}- \\tilde{\\gamma} \\frac{g_t}{\\sqrt{state\\_sum_t}+\\epsilon}            \\\\\n",
    "    &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n",
    "    &\\bf{return} \\:  \\theta_t                                                     \\\\[-1.ex]\n",
    "    &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "* params (iterable): iterable of parameters to optimize or dicts defining parameter groups\n",
    "* lr (float, optional): learning rate (default: 1e-2)\n",
    "* lr_decay (float, optional): learning rate decay (default: 0)\n",
    "* weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n",
    "* eps (float, optional): term added to the denominator to improve numerical stability (default: 1e-10)\n",
    "* foreach (bool, optional): whether foreach implementation of optimizer is used (default: None)\n",
    "* maximize (bool, optional): maximize the params based on the objective, instead of minimizing (default: False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
