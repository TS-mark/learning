{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "optimizer = torch.optim.adam()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 优点：适应性学习率，适合处理稀疏数据，可以适应不同参数的不同更新频率  \n",
    "* 缺点：学习率可能在训练过程中过早减小，导致收敛缓慢或停止。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "            &\\rule{110mm}{0.4pt}                                                                 \\\\\n",
    "            &\\textbf{input}      : \\gamma \\text{ (lr)}, \\beta_1, \\beta_2\n",
    "                \\text{ (betas)},\\theta_0 \\text{ (params)},f(\\theta) \\text{ (objective)}          \\\\\n",
    "            &\\hspace{13mm}      \\lambda \\text{ (weight decay)},  \\: \\textit{amsgrad},\n",
    "                \\:\\textit{maximize}                                                              \\\\\n",
    "            &\\textbf{initialize} :  m_0 \\leftarrow 0 \\text{ ( first moment)},\n",
    "                v_0\\leftarrow 0 \\text{ (second moment)},\\: \\widehat{v_0}^{max}\\leftarrow 0\\\\[-1.ex]\n",
    "            &\\rule{110mm}{0.4pt}                                                                 \\\\\n",
    "            &\\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}                         \\\\\n",
    "\n",
    "            &\\hspace{5mm}\\textbf{if} \\: \\textit{maximize}:                                       \\\\\n",
    "            &\\hspace{10mm}g_t           \\leftarrow   -\\nabla_{\\theta} f_t (\\theta_{t-1})         \\\\\n",
    "            &\\hspace{5mm}\\textbf{else}                                                           \\\\\n",
    "            &\\hspace{10mm}g_t           \\leftarrow   \\nabla_{\\theta} f_t (\\theta_{t-1})          \\\\\n",
    "            &\\hspace{5mm}\\textbf{if} \\: \\lambda \\neq 0                                           \\\\\n",
    "            &\\hspace{10mm} g_t \\leftarrow g_t + \\lambda  \\theta_{t-1}                            \\\\\n",
    "            &\\hspace{5mm}m_t           \\leftarrow   \\beta_1 m_{t-1} + (1 - \\beta_1) g_t          \\\\\n",
    "            &\\hspace{5mm}v_t           \\leftarrow   \\beta_2 v_{t-1} + (1-\\beta_2) g^2_t          \\\\\n",
    "            &\\hspace{5mm}\\widehat{m_t} \\leftarrow   m_t/\\big(1-\\beta_1^t \\big)                   \\\\\n",
    "            &\\hspace{5mm}\\widehat{v_t} \\leftarrow   v_t/\\big(1-\\beta_2^t \\big)                   \\\\\n",
    "            &\\hspace{5mm}\\textbf{if} \\: amsgrad                                                  \\\\\n",
    "            &\\hspace{10mm}\\widehat{v_t}^{max} \\leftarrow \\mathrm{max}(\\widehat{v_t}^{max},\n",
    "                \\widehat{v_t})                                                                   \\\\\n",
    "            &\\hspace{10mm}\\theta_t \\leftarrow \\theta_{t-1} - \\gamma \\widehat{m_t}/\n",
    "                \\big(\\sqrt{\\widehat{v_t}^{max}} + \\epsilon \\big)                                 \\\\\n",
    "            &\\hspace{5mm}\\textbf{else}                                                           \\\\\n",
    "            &\\hspace{10mm}\\theta_t \\leftarrow \\theta_{t-1} - \\gamma \\widehat{m_t}/\n",
    "                \\big(\\sqrt{\\widehat{v_t}} + \\epsilon \\big)                                       \\\\\n",
    "            &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n",
    "            &\\bf{return} \\:  \\theta_t                                                     \\\\[-1.ex]\n",
    "            &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n",
    "       \\end{aligned}\n",
    "\n",
    "$$\n",
    "\n",
    "* params (iterable): 可迭代的参数以优化或定义参数组\n",
    "* lr (float, optional): 学习率 (default: 1e-3)\n",
    "* betas (Tuple[float, float], optional): 用于计算梯度及其平方的运行平均值的系数 (default: (0.9, 0.999))\n",
    "* eps (float, optional): 添加到分母中以提高数值稳定性的项 (default: 1e-8)\n",
    "* weight_decay (float, optional): 权重衰减 (L2 penalty) (default: 0)\n",
    "* amsgrad (bool, optional): 是否使用论文 `On the Convergence of Adam and Beyond`中AMSGrad变体    (default: False)\n",
    "* foreach (bool, optional): whether foreach implementation of optimizer is used (default: None)\n",
    "* maximize (bool, optional): maximize the params based on the objective, instead of\n",
    "    minimizing (default: False)\n",
    "* capturable (bool, optional): whether this instance is safe to capture in a CUDA graph \n",
    "    Passing True can impair ungraphed performance, so if you don't intend to\n",
    "    graph capture this instance, leave it False (default: False)\n",
    "* fused (bool, optional): whether fused implementation of optimizer is used. Currently, `torch.float64`, `torch.float32`, `torch.float16`, and `torch.bfloat16` are supported. (default: False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
