{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 该类实现 Rprop 优化方法(弹性反向传播)，适用于 full-batch，不适用于 mini-batch，因而在 mini-batch 大行其道的时代里，很少见到。\n",
    "\n",
    "优点：可以自动调节学习率，不需要人为调节  \n",
    "缺点：依赖于人工设置的一个全局学习率，随着迭代次数过多，学习率会越来越小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Rprop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\n",
    "\\begin{aligned}\n",
    "    &\\rule{110mm}{0.4pt}                                                                 \\\\\n",
    "    &\\textbf{input}      : \\theta_0 \\in \\mathbf{R}^d \\text{ (params)},f(\\theta)\n",
    "        \\text{ (objective)},                                                             \\\\\n",
    "    &\\hspace{13mm}      \\eta_{+/-} \\text{ (etaplus, etaminus)}, \\Gamma_{max/min}\n",
    "        \\text{ (step sizes)}                                                             \\\\\n",
    "    &\\textbf{initialize} :   g^0_{prev} \\leftarrow 0,\n",
    "        \\: \\eta_0 \\leftarrow \\text{lr (learning rate)}                                   \\\\\n",
    "    &\\rule{110mm}{0.4pt}                                                                 \\\\\n",
    "    &\\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}                         \\\\\n",
    "    &\\hspace{5mm}g_t           \\leftarrow   \\nabla_{\\theta} f_t (\\theta_{t-1})           \\\\\n",
    "    &\\hspace{5mm} \\textbf{for} \\text{  } i = 0, 1, \\ldots, d-1 \\: \\mathbf{do}            \\\\\n",
    "    &\\hspace{10mm}  \\textbf{if} \\:   g^i_{prev} g^i_t  > 0                               \\\\\n",
    "    &\\hspace{15mm}  \\eta^i_t \\leftarrow \\mathrm{min}(\\eta^i_{t-1} \\eta_{+},\n",
    "        \\Gamma_{max})                                                                    \\\\\n",
    "    &\\hspace{10mm}  \\textbf{else if}  \\:  g^i_{prev} g^i_t < 0                           \\\\\n",
    "    &\\hspace{15mm}  \\eta^i_t \\leftarrow \\mathrm{max}(\\eta^i_{t-1} \\eta_{-},\n",
    "        \\Gamma_{min})                                                                    \\\\\n",
    "    &\\hspace{15mm}  g^i_t \\leftarrow 0                                                   \\\\\n",
    "    &\\hspace{10mm}  \\textbf{else}  \\:                                                    \\\\\n",
    "    &\\hspace{15mm}  \\eta^i_t \\leftarrow \\eta^i_{t-1}                                     \\\\\n",
    "    &\\hspace{5mm}\\theta_t \\leftarrow \\theta_{t-1}- \\eta_t \\mathrm{sign}(g_t)             \\\\\n",
    "    &\\hspace{5mm}g_{prev} \\leftarrow  g_t                                                \\\\\n",
    "    &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n",
    "    &\\bf{return} \\:  \\theta_t                                                     \\\\[-1.ex]\n",
    "    &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n",
    "\\end{aligned}\n",
    " $$\n",
    "For further details regarding the algorithm we refer to the paper\n",
    "`A Direct Adaptive Method for Faster Backpropagation Learning: The RPROP Algorithm\n",
    "<http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.21.1417>`_.\n",
    "\n",
    "### Args:\n",
    "* params (iterable): iterable of parameters to optimize or dicts defining\n",
    "    parameter groups\n",
    "* lr (float, optional): learning rate (default: 1e-2)\n",
    "* etas (Tuple[float, float], optional): pair of (etaminus, etaplis),是乘法增加和减少因子（默认值：(0.5, 1.2)）  \n",
    "    * etaplus和etaminus是RPROP算法中的两个参数。etaplus表示权重更新值在导数为正时的增加因子，而etaminus表示权重更新值在导数为负时的减小因子。\n",
    "* step_sizes (Tuple[float, float], optional):  一对允许的最小和最大步长（默认值：(1e-6, 50)）\n",
    "* foreach (bool, optional): whether foreach implementation of optimizer\n",
    "    is used (default: None)\n",
    "* maximize (bool, optional): maximize the params based on the objective, instead of\n",
    "    minimizing (default: False)\n",
    "\n",
    "   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
