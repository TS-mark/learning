{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "虽然这里叫SGD，但实际上是实现的BGD，如果需要完全使用SGD,则将batch_size设置为1即可\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    &\\rule{110mm}{0.4pt}                                                                 \\\\\n",
    "    &\\textbf{input}      : \\gamma \\text{ (lr)}, \\: \\theta_0 \\text{ (params)}, \\: f(\\theta)\n",
    "        \\text{ (objective)}, \\: \\lambda \\text{ (weight decay)},                          \\\\\n",
    "    &\\hspace{13mm} \\:\\mu \\text{ (momentum)}, \\:\\tau \\text{ (dampening)},\n",
    "    \\:\\textit{ nesterov,}\\:\\textit{ maximize}                                     \\\\[-1.ex]\n",
    "    &\\rule{110mm}{0.4pt}                                                                 \\\\\n",
    "    &\\textbf{for} \\: t=1 \\: \\textbf{to} \\: \\ldots \\: \\textbf{do}                         \\\\\n",
    "    &\\hspace{5mm}g_t           \\leftarrow   \\nabla_{\\theta} f_t (\\theta_{t-1})           \\\\\n",
    "    &\\hspace{5mm}\\textbf{if} \\: \\lambda \\neq 0                                           \\\\\n",
    "    &\\hspace{10mm} g_t \\leftarrow g_t + \\lambda  \\theta_{t-1}                            \\\\\n",
    "    &\\hspace{5mm}\\textbf{if} \\: \\mu \\neq 0                                               \\\\\n",
    "    &\\hspace{10mm}\\textbf{if} \\: t > 1                                                   \\\\\n",
    "    &\\hspace{15mm} \\textbf{b}_t \\leftarrow \\mu \\textbf{b}_{t-1} + (1-\\tau) g_t           \\\\\n",
    "    &\\hspace{10mm}\\textbf{else}                                                          \\\\\n",
    "    &\\hspace{15mm} \\textbf{b}_t \\leftarrow g_t                                           \\\\\n",
    "    &\\hspace{10mm}\\textbf{if} \\: \\textit{nesterov}                                       \\\\\n",
    "    &\\hspace{15mm} g_t \\leftarrow g_{t} + \\mu \\textbf{b}_t                             \\\\\n",
    "    &\\hspace{10mm}\\textbf{else}                                                   \\\\[-1.ex]\n",
    "    &\\hspace{15mm} g_t  \\leftarrow  \\textbf{b}_t                                         \\\\\n",
    "    &\\hspace{5mm}\\textbf{if} \\: \\textit{maximize}                                          \\\\\n",
    "    &\\hspace{10mm}\\theta_t \\leftarrow \\theta_{t-1} + \\gamma g_t                   \\\\[-1.ex]\n",
    "    &\\hspace{5mm}\\textbf{else}                                                    \\\\[-1.ex]\n",
    "    &\\hspace{10mm}\\theta_t \\leftarrow \\theta_{t-1} - \\gamma g_t                   \\\\[-1.ex]\n",
    "    &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n",
    "    &\\bf{return} \\:  \\theta_t                                                     \\\\[-1.ex]\n",
    "    &\\rule{110mm}{0.4pt}                                                          \\\\[-1.ex]\n",
    "\\end{aligned}\n",
    "       \n",
    "$$\n",
    "\n",
    "参数：\n",
    "* params：可迭代的参数以优化或定义参数组\n",
    "* lr：学习率\n",
    "* momentum：动量因子\n",
    "* weight_decay:权重衰减（L2惩罚）\n",
    "* dampening:动量阻尼\n",
    "* nesterov:启用nesterov动量\n",
    "* maximize:根据目标最大化参数（默认最小化，梯度下降嘛）\n",
    "* foreach:\n",
    "    是否使用foreach实现，一般别管这个\n",
    "* differentiable：\n",
    "    （没太理解）是否应通过训练中的优化器步骤进行 autograd。否则，step() 函数在 torch.no_grad() 上下文中运行。设置为 True 会影响性能，因此如果您不打算通过此实例运行 autograd，请将其保留为 False（默认值：False）\n",
    "\n",
    "```python\n",
    "# 使用参考\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = lr, momentum = 0.9, weight_decay = 5e-4)\n",
    "```\n",
    "\n",
    "Nesterov momentum is based on the formula from\n",
    "`On the importance of initialization and momentum in deep learning`__.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里有必要解释下动量：\n",
    "![](./img/SGD1.png)\n",
    "添加部分$\\mu$是指增加动量模块，$\\tau$是阻尼系数，动量$b_t$初始化值是$g_t$ ,就是第一次梯度，之后更新结果为:$\\mu b_{t-1}[动量更新部分] + (1-\\tau)g_t[阻尼部分]$\n",
    "\n",
    "<font color = \"red\">这里我的通俗理解,动量就是把上一次的梯度传下去计算了</font>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解读一下，这个公式可以写成这样：\n",
    "\n",
    "$$ \\theta_t \\leftarrow \\theta_{t-1} - \\gamma (\\nabla_\\theta f_t(\\theta_{t-1}) +\\lambda \\theta_{t-1}+\\mu g_{t-1} ) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
