agent  --> 代表智能体
MDP --> 马尔可夫决策过程

强化学习是一种强大的学习算法，无需环境模型，通过与环境的交互来学习最优策略
[J. Kober, J. A. Bagnell, and J. Peters, ‘‘Reinforcement learning in robotics: A survey,’’ Int. J. Robot. Res., vol. 32, no. 11, pp. 1238–1274, 2013.]
它使用通过与环境交互来学习给定策略的价值函数的代理来预测最佳解决方案，并基于价值函数不断开发和学习最佳策略  

Q-learning 具有简单的 Q 函数，因此它已成为许多其他强化学习算法的基础[10]

MDP是顺序动作决策问题的数学定义  

环境是概率性的，这意味着执行动作后转换的状态和补偿是随机的。 在特定状态下选择要执行的动作的规则称为策略，可以使用 MDP 制定强化学习算法  
（1）STATE （状态）
状态是代理可观察状态的集合 S。 状态意味着“观察你的情况”
（2）ACTION  
一个动作是在状态S下的一组可能的动作A。通常，一个agent在所有状态下可以做的动作都是相同的。  
（3） STATE TRANSITION PROBABILITY MATRIX（状态转移矩阵）
状态转移概率是智能体在采取行动 A 时从一个状态 S 移动到另一个状态 S ' 的数值表示
$$ P_{SS`}^a = P[S_{t+1}=s`|S_t = s, A_t = a] $$  
(4) REWARD  
奖励是在环境中给予智能体的信息，以便智能体可以学习。 当状态为 s 且动作为 a 在时间 t 时，agent 收到的奖励为：
$$R_{SS`}^a = E[R_{t+1}|S_t = s, A_t = a]$$  
(5) DISCOUNT FACTOR(折扣因子)
在强化学习中，discount factor（折扣因子）是一个介于0到1之间的实数，通常表示为 $\gamma$ 。它用于衡量未来奖励的重要性相对于当前奖励的重要性。  
具体来说，当一个智能体在环境中采取一系列动作，每个动作都会产生相应的奖励。然而，智能体面临的决策问题通常是长期的，即在未来的时间步中采取的动作也会影响奖励的总和。因此，我们需要考虑如何平衡当前和未来奖励的价值。 
(6) POLICY  
$$\pi (a|s)= P[A_t = a|S_t = s]$$  
