### state 状态

### Action 动作

### Agent 智能体 -> 马里奥或者汽车

### policy 策略 $ \pi $
policy是个概率密度函数
$$\pi (s,a) = P(A = a|S = s)$$

### reward 奖励R
定义奖励是强化学习重要的一环，目标是让奖励高。

$$U_t = R_t + R_{t+1} + R_{t+2} + R_{t+3}+...$$
#### 折扣奖励（折扣因子）$\gamma$
$$U_t = R_t + \gamma R_{t+1} + \gamma^2 R_{t+2} + \gamma^3 R_{t+3}+...$$

### state transition 状态转移

### trajectory(state,action,reward) 轨迹
$$ s_1,a_1,r_1,s_2,a_2,r_2,...,s_T,a_T,r_T $$

### Action-value function
对于策略$\pi$的动作价值函数
$Q_\pi$会评估agent在状态s前提下采取动作a的策略分数（在策略π的前提下）
$$Q_\pi (s_t,a_t) = E[U_t|S_t = s_t, A_t = a_t]$$

Optimal action-value function:
(这个很重要)
$$Q^* (s_t, a_t) = \max_\pi (s_t,a_t)$$

### state-value function
策略价值函数:
如果离散
$$V_\pi (s_t) = E_A [Q_\pi (s_t,A)] = \sum_a\pi(a|s_t)\cdot Q_\pi (s_t,a)$$
如果连续
$$V_\pi (s_t) = E_A [Q_\pi (s_t,A)] = \int_a\pi(a|s_t)\cdot Q_\pi (s_t,a) \text{d}a$$
Vπ是对Qπ关于a的积分，当前状态s越好，Vπ数值越大。  
Vπ可以评价π的好坏


强化学习主要来学习π函数或者Q*函数，分别为策略学习，价值学习

```python
import gym
env = gym.make('CartPole-v0')
state = env.reset()
for t in range(100):
    env.render()
    print(state)

    action = env.action_sppace.sample()
    state, reward , done, info = env.step(action)

    if done:
        print('Finished')
        break

env.close()
```